{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment 05 - ANN, PV, and NER.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"TnoWgLHH37Ro","colab_type":"text"},"source":["# Assignment 05 - ANN, PV, and NER\n","In this laboratory we will explore so some aspects of Artificial Neural Networks (ANN), Paragraph Vectors (PV), and Named Entity Recognition (NER)"]},{"cell_type":"code","metadata":{"id":"SsmyEgOS85vq","colab_type":"code","colab":{}},"source":["!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1sGMKT-IUhx1K-im9BCUWDIGQ78EN9Sed\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1sGMKT-IUhx1K-im9BCUWDIGQ78EN9Sed\" -o fdata.zip\n","\n","!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" -o bdata.zip\n","\n","!unzip -q bdata.zip\n","!unzip -q fdata.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qc7_S5HO37Rq","colab_type":"text"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"ib6DPRmk37Rr","colab_type":"text"},"source":["## Part A"]},{"cell_type":"markdown","metadata":{"id":"UE27SJ0H37Rr","colab_type":"text"},"source":["1. Experiment with the network structure that works best for your binary classification dataset.  Explore the following choices: \n","\n","    a.) number of layers in the MLP;\n","    \n","    b.) the size of each layer; \n","    \n","    c.) the activation functions; \n","    \n","    d.) the use of dropout.  \n","    \n","    *Hint: Take a look at the [tensorflow keras](https://www.tensorflow.org/api_docs/python/tf/keras) documentation to see the allowed values and parameters*"]},{"cell_type":"markdown","metadata":{"id":"HspuG9Jd37Rs","colab_type":"text"},"source":["2. Which architecture performs best on the development data?  \n","    * Create 5 different models and execute them below.  One of the models should be logistic regression (i.e., an MLP with *no* hidden layers).\n","    * Use the functions from Part A (`train_and_evaluate_real`)"]},{"cell_type":"code","metadata":{"id":"f-f2kS8Q37Rt","colab_type":"code","colab":{}},"source":["def get_model1():\n","    model = tf.keras.model.Sequential()\n","    # your model here\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","        \n","    return model\n","\n","model1=get_model1()\n","dev_accuracy, test_accuracy=train_and_evaluate_real(model1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1H_neGeO37Ry","colab_type":"code","colab":{}},"source":["def get_model2():\n","    model = Sequential()\n","    # your model here\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","        \n","    return model\n","\n","model2=get_model2()\n","dev_accuracy, test_accuracy=train_and_evaluate(model2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbYZK33937R2","colab_type":"code","colab":{}},"source":["def get_model3():\n","    model = Sequential()\n","    # your model here\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","        \n","    return model\n","\n","model3=get_model3()\n","dev_accuracy, test_accuracy=train_and_evaluate(model3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LwWIvZF437R5","colab_type":"code","colab":{}},"source":["def get_model4():\n","    model = Sequential()\n","    # your model here\n","\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","        \n","    return model\n","\n","model4=get_model4()\n","dev_accuracy, test_accuracy=train_and_evaluate(model4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uayxzm2-37R7","colab_type":"code","colab":{}},"source":["def get_logreg():\n","    model = Sequential()\n","\n","   # your model here\n","        \n","    return model\n","    \n","logreg=get_logreg()\n","dev_accuracy, test_accuracy=train_and_evaluate(logreg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zPfWTgv437R9","colab_type":"text"},"source":["3. For the single best model you identified in (1/2) above, calculate 95% confidence intervals it makes on the test data."]},{"cell_type":"markdown","metadata":{"id":"2Wwr3MGH37R9","colab_type":"text"},"source":["4. Unlike logistic/linear regression, neural networks converge to different solutions as a function of their *initialization* (the random choice of the initial values for parameters).  For the best model that's not logistic regression you identified in Question 1 and 2 above, train the model 10 times and save the accuracies attained on the development data.  Plot the distribution of dev accuracies using [pandas.DataFrame.plot.density](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html). "]},{"cell_type":"markdown","metadata":{"id":"8xfS4K-h37R9","colab_type":"text"},"source":["## Part B"]},{"cell_type":"markdown","metadata":{"id":"4jLgpjZV37R-","colab_type":"text"},"source":["Train your model using the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) (`lee_background.cor`) included in `data/fdata/lee`. \n","\n","This corpus contains 300 documents selected from the Australian Broadcasting Corporationâ€™s news mail service, which provides text e-mails of headline stories and covers a number of broad topics.\n","\n","We will test how our training model relates to a document from our test corpus (`lee.cor`) included in `data/fdata/lee`.\n","\n","1. For this, create your own pre-processing task to clean your data. You can inlcude any desired routine, as long as the documents in your corpus are represented as a list of tokens. \n","\n","    * Doc2vec requires a list of tokens for its training routine.\n","    * For teh training step, do2vec needs ids for its document, use can a simple counter for this\n","    * Pre-process both `lee.cor` and `lee_brackground` and train 2 different models using the latter.\n","    * Make sure to use different parameters for each model (the more distintic the beter)\n","    * Save both models in a binary fashin\n","    * Clean your run/execution\n","    \n","\n","2. For each model you trained, load them as separate instances. Since they will be small. you can load them in the same run. In case you have any memory problem, run one at a time.\n","\n","\n","3. Use the function `infer_vector` and convert 2 random documents from your test set (`lee.cor`). After that, look for their respective most similar documents in your trained models for the `top-K` documents (`K`).\n","\n","    * Take a look on the function `most_similar`\n","    * Show how the different parameters can affect your document-vector inference. Use two different hyperparameter configuration when inferring the vectors for your random-test document.\n","\n","\n","4. (Optional) Take one document from the test collection, infer its vectors (pick the configuration that gave you the highest similarity value from the previous exercise).\n","\n","    * Provide the most similar document,the least, and the median similar documents from your training set. *Hint: You will need to consider all documents in your training corpus/model*.\n","\n"]},{"cell_type":"code","metadata":{"id":"8Ty7_FYs37R-","colab_type":"code","colab":{}},"source":["# Set file names for train and test data\n","lee_train_file = './fdata/lee/lee_background.cor'\n","lee_test_file = './fdata/lee/lee.cor'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tm2aDdUB37SA","colab_type":"text"},"source":["## Part C"]},{"cell_type":"markdown","metadata":{"id":"bAV404sT37SA","colab_type":"text"},"source":["1. Improve our function `resolve_toponyms` so we can reduce the wrong marks in the map."]},{"cell_type":"code","metadata":{"id":"YUJBR7af37SA","colab_type":"code","colab":{}},"source":["import spacy\n","from collections import defaultdict\n","import networkx as nx #for the optional part\n","import matplotlib.pyplot as plt  #for the optional part"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Wvj7wVH37SC","colab_type":"code","colab":{}},"source":["nlp = spacy.load('en_core_web_sm', disable=['parser'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_IsFjB_A37SD","colab_type":"code","colab":{}},"source":["def process(filename):\n","    with open(filename, encoding=\"utf-8\") as file:\n","        data=file.read()\n","        return nlp(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBp2p0Rz37SE","colab_type":"text"},"source":["2. Pick an English-language book you know from [Project Gutenberg](https://www.gutenberg.org) and save it in the `data/bdata/` directory.  Read it in here.  What two characters have a strong relationship in this book?"]},{"cell_type":"code","metadata":{"id":"Jv40jyCi37SE","colab_type":"code","colab":{}},"source":["# Default is Austen's Pride and Prejudice\n","doc=process(\"./bdata/pride_and_prejudice.txt\") # for example - You can pick another one if you want"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gZU-gquK37SG","colab_type":"text"},"source":["3. Your main task here will be to create a social network of people mentioned in text.  You will need to implement the following two functions: `get_nodes`, which returns a list of characters in a text along with a weight for them (e.g., their frequency of mention in the text) and `get_edges`, which returns a list of positive weights between those character nodes (if two characters do not have a tie between them, their weight is 0 and you don't have to include them in the edge list).\n","\n","* The interesting question here is how you measure whether a social tie exists between two characters in a text, and how you go about placing a weight on that edge that measures the strength of the tie. Two characters that have a strong tie should have a high weight.  \n","* Consider the different ways that we might measure social interaction in text -- the frequency with which two characters are mentioned together, how often they mention each other in dialogue, how \"friendly\" their interaction seems, etc.\n","    * For example, if there is a special words between two characters\n","\n","*Some reference reading*, see Elson et al. (2010) \"[Extracting Social Networks from Literary Texts](http://www1.cs.columbia.edu/~delson/pubs/ACL2010-ElsonDamesMcKeown.pdf)\" and Stiller et al. 2003, \"[The Small World of Shakespeare's Plays](http://www.staff.ncl.ac.uk/daniel.nettle/shakespeare.pdf)\"."]},{"cell_type":"markdown","metadata":{"id":"teoLRYdR37SG","colab_type":"text"},"source":["3.  How you are defining a social tie, and how you are measuring it in text?"]},{"cell_type":"markdown","metadata":{"id":"NJhtMc-237SG","colab_type":"text"},"source":["4. Implement `get_people_mentions`"]},{"cell_type":"code","metadata":{"id":"pPiVuKJM37SH","colab_type":"code","colab":{}},"source":["def get_people_mentions(doc, min_count=10):\n","    \"\"\" Extract all of the PERSON mentions in a spacy-processed document.\n","    Returns a dict mapping each unique person name to a list of spacy entity mentions\n","    Each spacy entity has the following attributes:\n","    \n","    * text\n","    * start position in document (character)\n","    * end position in document (character)\n","    * label (NER category)\n","    \n","    https://spacy.io/usage/linguistic-features#named-entities\n","    \"\"\"\n","    # your code here\n","    \n","    return people"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uUHw8zew37SI","colab_type":"text"},"source":["5. Implement `get_edges`"]},{"cell_type":"code","metadata":{"id":"QkzWE_GN37SI","colab_type":"code","colab":{}},"source":["def get_edges(people, doc):\n","    \"\"\" Creates network edges from a dict of people mentions and the full spacy-processed document\n","    Input: a dict of people mentions returned from get_people_mentions() and document returned from process()\n","    Output: a dict mapping a person all of the other people they are connected to, along with the weight of\n","    that connection.\n","    \n","    e.g., {\"Tom: {\"Huck\": 2, \"Sally\": 1}, \"Huck\": {\"Sally\": 1}}\n","    \n","    Keep in mind that doc gives you access to *all* of the tokens in the book.\n","    \n","    \"\"\"\n","    \n","     # your code here\n","    \n","    return edges"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"embtR8lI37SJ","colab_type":"text"},"source":["7. Show the the relationship for every *Person* in the document you picked. You can represent it as dict mapping a person to all of the other people they are connected to, along with the weight of that connection (e.g., count).\n","    * For example:    {\"Tom: {\"Huck\": 2, \"Sally\": 1}, \"Huck\": {\"Sally\": 1}} (basically the ouput from the previous function)"]},{"cell_type":"code","metadata":{"id":"oN7BUQGe37SK","colab_type":"code","colab":{}},"source":["people=get_people_mentions(doc) #extract all people from the book"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5s1mGXmw37SL","colab_type":"code","colab":{}},"source":["edges=get_edges(people, doc)# extract what are the people connected to each person in the book"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3wCUOuG37SL","colab_type":"text"},"source":["8. (Optional) Plot the social network graph composed by the people in the book.\n","    * Implement `get_nodes` below. You need to calculate how many times each person appears in the book to differentiate their importance for plotting"]},{"cell_type":"code","metadata":{"id":"GANDsRxs37SM","colab_type":"code","colab":{}},"source":["def get_nodes(people, min_count=10):\n","    \"\"\" Creates network nodes from a dict of people mentions\n","    Input: a dict of people mentions returned from get_people_mentions()\n","    Output: a dict mapping the entity name to a positive numerical value of their importance \n","    (the size of the node in a network graph)\n","    \n","    e.g., {\"Tom\": 5, \"Huck\": 1}\n","    \n","    \"\"\"\n","     # your code here\n","    \n","    return nodes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dng0vO8k37SN","colab_type":"code","colab":{}},"source":["# (Optional) You need to calculate how many times each person appears in the book to differentiate their importance\n","nodes=get_nodes(people) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8W2HQ0337SO","colab_type":"text"},"source":["9. (Optional) Implement the `create_graph` function. This will actualy show the graph considering the calulated nodes and edges"]},{"cell_type":"code","metadata":{"id":"DLAxZA6d37SP","colab_type":"code","colab":{}},"source":["def create_graph(nodes, edges):\n","\n","    \"\"\" Plot a set of weighted nodes and weighted edges on a network graph \"\"\"\n","    \n","    # increase this to expand network\n","    force_directed_expansion=2\n","    \n","    # increase these dimensions to make graph bigger\n","    figure_height=20\n","    figure_width=20\n","    \n","    G = nx.Graph()\n","    for person in nodes:\n","        G.add_node(person, nodesize=nodes[person])\n","    for person1 in edges:\n","        for person2 in edges[person1]:\n","            if person1 in nodes and person2 in nodes:\n","                G.add_weighted_edges_from([(person1, person2, edges[person1][person2]) ])\n","\n","    options = {\n","    'edgecolors':\"black\",\n","    'linewidths':1,\n","    'with_labels': True,\n","    'font_weight': 'regular',\n","    }\n","    \n","    g_edges = G.edges()\n","\n","    sizes = [G.node[node]['nodesize']*100000 for node in G]\n","    weights = [G[u][v]['weight']*10 for u,v in g_edges]\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(figure_height, figure_width));\n","\n","    nx.draw_networkx(G, pos=nx.spring_layout(G, k=force_directed_expansion, iterations=100), node_size=sizes, width=weights, **options)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCABEnlT37SQ","colab_type":"code","colab":{}},"source":["create_graph(nodes, edges)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRfztNmQ37SR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6maAZMR37SS","colab_type":"text"},"source":["##### Acknowledgements\n","Radim Å˜ehÅ¯Å™ek - RaRe Technologies - gensim\n","\n","David Bamman - University of California Berkeley\n","\n","Jason Xie - ?"]},{"cell_type":"code","metadata":{"id":"Kv-Bypqo37SS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}