{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment 04 - Word Embeddings and Lexical Resources.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"OGOcyvmFtx7w","colab_type":"text"},"source":["# Assignment 04 - Word Embeddings and Lexical Resources\n","This assignment explore words embeddings and lexical databases.\n","\n","As always we get some data that can be useful."]},{"cell_type":"code","metadata":{"id":"obmHZW2EuAdL","colab_type":"code","colab":{}},"source":["!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" -o bdata.zip\n","!unzip -q bdata.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cg8PNAI6tx7y","colab_type":"text"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"GYsHWS9Rtx70","colab_type":"text"},"source":["## Part A"]},{"cell_type":"markdown","metadata":{"id":"uJf6Fcnqtx71","colab_type":"text"},"source":["1. Using a word2vec model find 4 word vectors that you can derive algebraic properties. similar to the king-man queen-woman example. You can use the same model we used in our Laboratory. \n"," \n","2. Using this saem word vector show if the algebraic properties indeed exist\n","   * Make sure to illustrate if the relationship sim((V1 + V4 - V2), V3) is present. Validate your results printing out the actual vector values.  \n","   * The results obtained through model.most_similar() do make sense?"]},{"cell_type":"markdown","metadata":{"id":"_GAvY8pRtx72","colab_type":"text"},"source":["3. Use a different training corpus from the one used in the previous exercise and train 2 different word2vec models and perform the same comparisons from (1) and (2). Make sure to use different parameters for each run (try to explore the different training objectives to see what happens)\n","    * just make sure to have one document-record per line, like in the `wiki.10K.txt` example-file. Suggestions:\n","       * [William Shakespeare](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt)\n","       * Top XK pages on wikipedia\n","       * [OpinRank Data](http://kavita-ganesan.com/entity-ranking-data/#.XclqXzNKg2x)\n","       * [IMDB Reviews](http://ai.stanford.edu/~amaas/data/sentiment/)\n","       * [Multi-Domain Sentiment Dataset](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/)\n","       * [Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)\n","       * [Yelp Open Dataset](https://www.yelp.com/dataset)\n","       * ...\n","       \n","      \n","Using a smaller or larger corpus affected your results? How?\n","\n","Make sure to include what parameters were used during your training phase.\n","\n","* minimum count, windows, epochs, mean/sum, etc. *Tip: create a table with the main differences between all the models/parameters/results/used*. Explain how do you think these hyperparameters influenced your results/model.\n","\n","Do not forget to provide the references/link from where did you get this new training corpus.\n","\n","* Submit your dataset so we can reproduce your results. If the data is too large, make it public available and share its link (compress it of course)."]},{"cell_type":"markdown","metadata":{"id":"NFniJBCxtx73","colab_type":"text"},"source":["## Part B"]},{"cell_type":"markdown","metadata":{"id":"p5A6g6oHtx75","colab_type":"text"},"source":["1. Using the pre-trained model from GloVe, find the same 4 vectors from Part (A - 1 and 2) and verify  semant relations exist in this new model.\n","    * It is most likely that the words that exist in the models used in **Part A** will exist here, but validate the existance of these vectors before you start (*maybe this notice should be up there, sorry*).\n","2. Why the values from the vector using the GloVe model are different?\n","    * What do you think affects the most your results?\n"]},{"cell_type":"markdown","metadata":{"id":"U7jr7t7Ltx76","colab_type":"text"},"source":["## Part C"]},{"cell_type":"markdown","metadata":{"id":"aswWz0sItx78","colab_type":"text"},"source":["1. Find all of the vehicles mentioned in Pride and Prejudice."]},{"cell_type":"markdown","metadata":{"id":"wwPNgo-Dtx78","colab_type":"text"},"source":["2. Find all of the verbs of speaking in Pride and Prejudice."]},{"cell_type":"markdown","metadata":{"id":"F65L_ogNtx79","colab_type":"text"},"source":["3. Find all of the people in Pride and Prejudice."]},{"cell_type":"markdown","metadata":{"id":"qVMxdq0atx7-","colab_type":"text"},"source":["##### Acknowledgements\n","\n","Radim Řehůřek - RaRe Technologies - [ensim](https://radimrehurek.com/gensim/models/word2vec.html)\n","\n","David Bamman - University of California Berkeley\n","\n","Jeffrey Pennington et. al. - Stanford - [GloVe](https://nlp.stanford.edu/projects/glove/)\n","\n","Mikolov et. al. - Google - word2vec [word2vec](https://code.google.com/archive/p/word2vec/)\n","\n"]},{"cell_type":"code","metadata":{"id":"Bsi38SROtx7_","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}