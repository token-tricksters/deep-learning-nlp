{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment 02b - Text Representation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"gYORZIBvHofi","colab_type":"text"},"source":["# Assignment 02b - Text Representation, Mining, and Feature Extraction"]},{"cell_type":"markdown","metadata":{"id":"sOd3wa5gHofj","colab_type":"text"},"source":["# Exercises\n","Make sure to provide all the cells/instructions to run your answers completely"]},{"cell_type":"markdown","metadata":{"id":"tbl8fwYBJLJn","colab_type":"text"},"source":["## Get The Data\n","\n","Before starting this tutorial we will need some text files to process. To make it as easy as possible, the following two lines will download and extract the necessary data.\n"]},{"cell_type":"code","metadata":{"id":"IVOfL1LSJL9U","colab_type":"code","colab":{}},"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1VRleNzl0RsHS43YekkBCMCUJZJB_GT6H' -O adata.zip\n","!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" -o bdata.zip\n","!unzip -q adata.zip\n","!unzip -q bdata.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-RcIfJGHofl","colab_type":"text"},"source":["## Part A"]},{"cell_type":"markdown","metadata":{"id":"EtpC4HFUHofm","colab_type":"text"},"source":["1. Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above (\"**Exercises Part A**\"). Examine those tweets; how would you characterize the differences?"]},{"cell_type":"markdown","metadata":{"id":"u0wG-Te9Hofn","colab_type":"text"},"source":["2. Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that appear in one, but don't appear in the other."]},{"cell_type":"code","metadata":{"id":"CHVxNWX0Hofp","colab_type":"code","colab":{}},"source":["compare(nltk_casual_tokens, nltk_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9KCDAYQHofw","colab_type":"text"},"source":["3. Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is."]},{"cell_type":"markdown","metadata":{"id":"F6eG30XJHofx","colab_type":"text"},"source":["4. Modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"]},{"cell_type":"code","metadata":{"id":"mEurK2NYHofx","colab_type":"code","colab":{}},"source":["print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://nlprocks.com/but/it/can/be#hard&complex\")))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KW1Bm9jyHof2","colab_type":"text"},"source":["## Part B\n","Load these cells right after **Exercises Part B** in the Laboratory notebook."]},{"cell_type":"markdown","metadata":{"id":"TEojrCQ9Hof3","colab_type":"text"},"source":["__HTRC Features__\n","1. What are the names of the books in the following files `adata/mdp.39015003763490.json.bz2` and `adata/mdp.39015005607976.json.bz2`? Do the results look a little odd? You can adjust the code in the print statement to tell you more about the volumes. *Hint: use vol.enumeration_chronology to see which part of a set the book is.*\n"]},{"cell_type":"markdown","metadata":{"id":"QmAC5U6PHof3","colab_type":"text"},"source":["2. There is an attribute of a volume that tells you how many pages a book has. It starts with a `p`, but we won't give you the whole thing - try TAB auto-completion on `vol.p` to find it!\n","\n","3. What information is returned by the attribute `vol.ht_bib_url`?\n","\n","4. `vol.line_chars()` is a method gives you a count of characters at the beginning and end of lines on a page. Try running in.\n","    - How does running a 'method' differ from an attribute like in the earlier questions?\n","    - What happens if you run this method as if it was an attribute?\n","    - Why do you think there is a distinction between attributes and methods?\n","    \n","5. We sneakily introduced a convenience above, when showing you the `vol_handle_url` - Jupyter automatically printed it without us telling it to `print()` as before. Can you guess - through tinkering - when Jupyter automatically prints? Try, for example, the following code block for a hint:\n","\n","```Python\n","vol.handle_url\n","vol.title\n","vol.year\n","```"]},{"cell_type":"markdown","metadata":{"id":"Fu2fkMIBHof4","colab_type":"text"},"source":["6. Let's think about two books, any you'd like to compare. Retrieve their ID and other 5 features and print them. Use the instruction below as a reference to retrieve the information about your volume.\n","\n","```Python\n","vol = FeatureReader(ids=['....']).\n","```"]},{"cell_type":"markdown","metadata":{"id":"9JOq4kHKHof5","colab_type":"text"},"source":["7. Load the volume for this copy of [Pride and Prejudice](https://babel.hathitrust.org/cgi/pt?id=hvd.32044089606552). Did it work? How many pages are there in the volume?\n","\n","8. Load a tokenlist of just the words in the header, not preserving case-sensitivity, or page or part of speech information. Set it to `tl` as we did above.\n","\n","    - How many unique words, or tokens, are there in the header? *Tip: scroll to the bottom of the table view*\n","    - How are the rows sorted?\n","    - What are the most common 'real' words among the tokens?\n","    \n","\n","9. `tl` has a sorting method, `tl.sort_values()`. Looking up the documentation, see if you can sort on the `count` column. \n","    - Adapt the above code to sort by 'count' in *descending* order.\n","    \n","    \n","10. Looking at word counts in page headers is uncommon - usually you want to ignore them and focus just on the body of the page! Retrieve the top words from Pride and Prejudice, again ignoring page, part of speech, and case information. After sorting, determine:\n","    - What types of tokens are most common overall? When are they useful?\n","    - Which occurs more - `he` or `she`?\n","    - Do we see any person names among the top words?\n","    \n","    \n","11. When do we want `case` information? When might we not want it?\n","\n","12. What does `page_freq=True` do when given as an argument to `vol.tokenlist()`? You can look at the documentation (`vol.tokenlist?`) or just try to infer from context.\n"]},{"cell_type":"markdown","metadata":{"id":"DDGQnLsKHof5","colab_type":"text"},"source":["__Extra__\n","13. Try to retrieve the five most-common tokens used as a noun ('NNP') or a plural noun ('NNS') in the book*. You will have to get a new tokenlist, without pages but with parts-of-speech, then slice by the criteria, sort, and output the first five rows."]},{"cell_type":"code","metadata":{"id":"d89RVAnOHof5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzLWgeSKHof9","colab_type":"text"},"source":["##### Acknowledgements\n","David Bamman - University of Berkeley California\n","\n","Hathi Trust Reseach Center\n","\n","Peter Organisciak - University of Denver\n","\n","St√©fan Sinclair - McGill University"]},{"cell_type":"code","metadata":{"id":"9_pMkZyiHof9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}