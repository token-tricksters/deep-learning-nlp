{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment 03 - BOW, Features and TF-IDF.ipynb","provenance":[],"collapsed_sections":["CwYVczJAJuKe","D8OArawwJuK7"],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"grLYsOFPJuJ6"},"source":["# Assignment 03 - BOW, Features and TF-IDF\n"]},{"cell_type":"markdown","metadata":{"id":"N4OE4gv8JuJ8"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"D3zC3aXfKRWB"},"source":["## Get The Data\n","\n","Before starting this tutorial we will need some text files to process. To make it as easy as possible, the following two lines will download and extract the necessary data.\n"]},{"cell_type":"code","metadata":{"id":"1jIbZtCtKSze"},"source":["!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" > /dev/null\n","!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1dNFLyLBK-0RkAu5Pzb_Yn9VghVl1Lxjf\" -o bdata.zip\n","!unzip -q bdata.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ9bjtQrJuJ9"},"source":["## Part A"]},{"cell_type":"markdown","metadata":{"id":"616iXZkYJuJ-"},"source":["Dictionaries can be use for counting the frequency of some category of words in text as we saw last class, using sentiment (from the [AFINN sentiment lexicon](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)) in the time series data of tweets as an example.\n","\n","This notebook uses data from the AFINN sentiment lexicon; for other dictionaries in wide use, see [MPQA](https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/) (free for use with registration) and [LIWC](http://liwc.wpengine.com) (commercial)."]},{"cell_type":"code","metadata":{"id":"t1MYE639JuJ_"},"source":["import json\n","import nltk\n","import pandas as pd #common alias used in Python\n","import matplotlib\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OSpN3UtbJuKG"},"source":["# function to read in json file of tweets and return a list of (date, tokenized text)\n","def read_tweets_from_json(filename):\n","    \n","    tweets=[]\n","    with open(filename, encoding=\"utf-8\") as file:\n","        data=json.load(file)\n","        for tweet in data:\n","            created_at=tweet[\"created_at\"]\n","            date = pd.to_datetime(created_at) #\n","            text=tweet[\"text\"]\n","            tokens=nltk.casual_tokenize(text)\n","            tweets.append((date, tokens))\n","    return tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SwofSKWJuKK"},"source":["# read in list of (date, tokens) tweets and count whether each tweet contains \n","# a (lowercased) term in the argument dictionary.  (BOW)\n","# Return as pandas dataframe for easier slicing/plotting)\n","def dictionary_document_count(data, dictionary):\n","    counted=[]\n","    for date, tokens in data:\n","        val=0\n","        for word in tokens:\n","            if word.lower() in dictionary:\n","                val=1\n","        counted.append((date, val))\n","    df=pd.DataFrame(counted, columns=['date','document frequency'])\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vuso8PIzJuKN"},"source":["tweets=read_tweets_from_json(\"./bdata/trump_tweets.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xgr_X1s2JuKR"},"source":["immigration_dictionary=set([\"wall\", \"border\", \"borders\", \"immigrants\",\"immigration\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4940pqkNJuKT"},"source":["def plot_time(counts):\n","    \n","    # for this exercise, let's just keep tweets published after 2015\n","    counts=counts[(counts['date'] > '2015-01-01')]\n","    \n","    # counts is a pandas dataframe; let's aggregate the counts by month.  \n","    # Can also aggregate by \"D\" for day, \"W\" for week, \"Y\" for year.\n","    means=counts.resample('M', on='date').mean() \n","    \n","    means.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_bkAHM2JuKV"},"source":["1. The AFINN dictionary is a sentiment lexicon, where words are rated on a five-point affect scale (-5 = most negative, 5 = most positive).  Write a function `read_AFINN_dictionary` to read in this file and create two dictionaries  (Exercises Part A from Lab 03) -- one for positive terms and one for negative terms.  \n","\n","How did you decide the cutoff point for positive and negative?"]},{"cell_type":"code","metadata":{"id":"A-Rt_GhKJuKV"},"source":["def read_AFINN_dictionary(filename):\n","   \n","    \n","    return set(positive), set(negative)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duh7P53zJuKX"},"source":["positive, negative=read_AFINN_dictionary(\"./bdata/AFINN-111.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JvK38RoJuKZ"},"source":["2. Create a plot using the negative sentiment dictionary you created."]},{"cell_type":"markdown","metadata":{"id":"Y8mg2oqhJuKa"},"source":["3. Create a new dictionary of your own for a concept you'd like to measure in `trump_tweets.json` or `aoc_tweets.json`.  The dictionary must contain at least 10 terms; you're free to create one for any category (except sentiment!).Create a plot using that dictionary and data."]},{"cell_type":"markdown","metadata":{"id":"dyD9QEBhJuKa"},"source":["4. (Extra) For each of the terms in your dictionary, write a function `print_examples(tweets, dictionary)` to find one tweet that contains that term and print it out for your inspection.  Is that term used in the same sense you expected?"]},{"cell_type":"code","metadata":{"id":"bTDAOBJjJuKa"},"source":["def print_examples(data, dictionary):\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"-FkijQY5JuKc"},"source":["print_examples(tweets, immigration_dictionary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwYVczJAJuKe"},"source":["## Part B"]},{"cell_type":"markdown","metadata":{"id":"ffylnMRVJuKe"},"source":["Feature engineering for text classification. Consider the data under ```data/bdata/text_classification_sample_data``` and the **Feature Exatraction** block."]},{"cell_type":"markdown","metadata":{"id":"Gjlo8n8CJuKe"},"source":["Your task is to create two new feature functions (like `dictionary_feature` and `unigram_feature` below), and include them in the `build_features` function."]},{"cell_type":"code","metadata":{"id":"gaCS-WvSJuKf"},"source":["import sys\n","from collections import Counter\n","from sklearn import preprocessing\n","from sklearn import linear_model\n","import pandas as pd\n","from scipy import sparse\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fcQISaHDJuKh"},"source":["def read_data(filename):\n","    X=[]\n","    Y=[]\n","    with open(filename, encoding=\"utf-8\") as file:\n","        for line in file:\n","            cols=line.rstrip().split(\"\\t\")\n","            label=cols[0]\n","            text=cols[1]\n","            X.append(text)\n","            Y.append(label)\n","    return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRdpLTPNJuKi"},"source":["# The directory should contain train.tsv, dev.tsv and test.tsv\n","directory=\"data/bdata/text_classification_sample_data\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mz8funS_JuKj"},"source":["1. Briefly describe your data (including the categories you're predicting)"]},{"cell_type":"code","metadata":{"id":"tXLdWcJfJuKk"},"source":["trainX, trainY=read_data(\"%s/train.tsv\" % directory)\n","devX, devY=read_data(\"%s/dev.tsv\" % directory)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TazjYirxJuKl"},"source":["def majority_class(trainY, devY):\n","    labelCounts=Counter()\n","    for label in trainY:\n","        labelCounts[label]+=1\n","    majority=labelCounts.most_common(1)[0][0]\n","    \n","    correct=0.\n","    for label in devY:\n","        if label == majority:\n","            correct+=1\n","            \n","    print(\"%s\\t%.3f\" % (majority, correct/len(devY)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxpHq9aXJuKm"},"source":["Here are two examples of features we've computed -- one feature class noting the presence of a word in an external dictionary, and one feature class for the word identity (i.e., unigram).  \n","\n","We'll implement each feature class as a function that takes a single document as input (as a list of tokens) and returns a dict corresponding to the feature we're creating."]},{"cell_type":"code","metadata":{"id":"foNGb_q5JuKn"},"source":["# Here's a sample dictionary we can create by inspecting the output of the Mann-Whitney test (in 2.compare/)\n","dem_dictionary=set([\"republican\",\"cut\", \"opposition\", \"Trump\"])\n","repub_dictionary=set([\"growth\",\"economy\", \"Hillary\"])\n","\n","def political_dictionary_feature(tokens):\n","    feats={}\n","    for word in tokens:\n","        if word in dem_dictionary:\n","            feats[\"word_in_dem_dictionary\"]=1\n","        if word in repub_dictionary:\n","            feats[\"word_in_repub_dictionary\"]=1\n","    return feats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBdq_GvIJuKo"},"source":["def unigram_feature(tokens):\n","    feats={}\n","    for word in tokens:\n","        feats[\"UNIGRAM_%s\" % word]=1\n","    return feats"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZSbXQ01lJuKq"},"source":["2. Add first new feature function here.  Describe your feature and why you think it will help."]},{"cell_type":"code","metadata":{"id":"kQGq-bLAJuKq"},"source":["def new_feature_class_one(tokens):\n","    feats={}\n","    feats[\"_FILL_IN_FEATURES_HERE_\"]=1\n","    return feats"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"onUTnA2LJuKs"},"source":["3. Add second new feature function here. Describe your feature and why you think it will help."]},{"cell_type":"code","metadata":{"id":"OBJieCnBJuKs"},"source":["def new_feature_class_two(tokens):\n","    feats={}\n","    feats[\"_FILL_IN_FEATURES_HERE_\"]=1\n","    return feats"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BvJoW80kJuKu"},"source":["This is the main function to aggregate together all of the information from different feature classes.  \n","\n","Each document has a feature dict (`feats`), and we'll update that dict with the new dict that each separate feature class is returning. *Hint:make sure that the keys each feature function is creating are unique (why?)*"]},{"cell_type":"code","metadata":{"id":"Dqs1YNvkJuKu"},"source":["def build_features(trainX, feature_functions):\n","    data=[]\n","    for doc in trainX:\n","        feats={}\n","\n","        # sample text data is already tokenized; if yours is not, do so here\n","        tokens=doc.split(\" \")\n","        \n","        #for each new function of in our feature-functions we will update our features\n","        for function in feature_functions:\n","            feats.update(function(tokens))\n","\n","        data.append(feats)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SEiUPKNJuKv"},"source":["# converts a dictionary of feature names to unique numerical ids\n","def create_vocab(data):\n","    feature_vocab={}\n","    idx=0\n","    for doc in data:\n","        for feat in doc:\n","            if feat not in feature_vocab:\n","                feature_vocab[feat]=idx\n","                idx+=1\n","                \n","    return feature_vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzwuw45GJuKw"},"source":["# converts a dictionary of feature names to a sparse representation\n","# we can fit in a scikit-learn model.  This is important because almost all feature \n","# values will be 0 for most documents (note: why?), and we don't want to save them all in \n","# memory.\n","\n","def features_to_ids(data, feature_vocab):\n","    new_data=sparse.lil_matrix((len(data), len(feature_vocab)))\n","    for idx,doc in enumerate(data):\n","        for f in doc:\n","            if f in feature_vocab:\n","                new_data[idx,feature_vocab[f]]=doc[f]\n","    return new_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tm8a-bImJuKy"},"source":["# This function evaluates a list of feature functions on the training/dev data arguments\n","def pipeline(trainX, devX, trainY, devY, feature_functions):\n","    trainX_feat=build_features(trainX, feature_functions)\n","    devX_feat=build_features(devX, feature_functions)\n","\n","    # just create vocabulary from features in *training* data\n","    feature_vocab=create_vocab(trainX_feat)\n","\n","    trainX_ids=features_to_ids(trainX_feat, feature_vocab)\n","    devX_ids=features_to_ids(devX_feat, feature_vocab)\n","    \n","    logreg = linear_model.LogisticRegression(C=1.0, solver='lbfgs', penalty='l2', max_iter=10000)\n","    logreg.fit(trainX_ids, trainY)\n","    print(\"Accuracy: %.3f\" % logreg.score(devX_ids, devY))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JC5PlGxJuKz"},"source":["majority_class(trainY,devY)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WK3sCgyVJuK0"},"source":["4. Explore the impact of different feature functions by evaluating them below:"]},{"cell_type":"code","metadata":{"id":"RISt9T8UJuK0"},"source":["features=[political_dictionary_feature]\n","pipeline(trainX, devX, trainY, devY, features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WViCkxsyJuK1"},"source":["features=[political_dictionary_feature, unigram_feature]\n","pipeline(trainX, devX, trainY, devY, features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRqG_TvEJuK3"},"source":["features=[new_feature_class_one]\n","pipeline(trainX, devX, trainY, devY, features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TIvbAexTJuK4"},"source":["features=[new_feature_class_two]\n","pipeline(trainX, devX, trainY, devY, features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9keQ4SpDJuK5"},"source":["features=[new_feature_class_one, new_feature_class_two]\n","pipeline(trainX, devX, trainY, devY, features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BrYHGlSyJuK6"},"source":["features=[unigram_feature, new_feature_class_one, new_feature_class_two]\n","pipeline(trainX, devX, trainY, devY, features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plYeABjCJuK7"},"source":["5. (Extra) If we did some preprocessing in our tokens, should we get new results?. Create a function to clean your data (e.g., stopword removal, stemm, lemmatization) and see if that affects your results"]},{"cell_type":"markdown","metadata":{"id":"D8OArawwJuK7"},"source":["## Part C"]},{"cell_type":"code","metadata":{"id":"KhuTuazsJuK8"},"source":["from collections import defaultdict, Counter\n","import math\n","import operator\n","import gzip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCUdxWuMJuK9"},"source":["window=2\n","vocabSize=10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCGmpSitJuK-"},"source":["filename=\"data/bdata/wiki.10K.txt\"\n","\n","wiki_data=open(filename, encoding=\"utf-8\").read().lower().split(\" \")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a90nmXvSJuK_"},"source":["# We'll only create word representation for the most frequent K words\n","\n","def create_vocab(data):\n","    word_representations={}\n","    vocab=Counter()\n","    for i, word in enumerate(data):\n","        vocab[word]+=1\n","\n","    topK=[k for k,v in vocab.most_common(vocabSize)]\n","    for k in topK:\n","        word_representations[k]=defaultdict(float)\n","    return word_representations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ks2J2BOxJuLA"},"source":["# word representation for a word = its unigram distributional context (the unigrams that show\n","# up in a window before and after its occurence)\n","\n","#`count_unigram_context` counts an individual unigram in the bag of words around a target as a \"context\" variable\n","\n","def count_unigram_context(data, word_representations):\n","    for i, word in enumerate(data):\n","        if word not in word_representations:\n","            continue\n","        start=i-window if i-window > 0 else 0\n","        end=i+window+1 if i+window+1 < len(data) else len(data)\n","        for j in range(start, end):\n","            if i != j:\n","                word_representations[word][data[j]]+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSkecXZ6JuLB"},"source":["# `count_directional_context` counts the sequence of words before and after the word as a single \n","# \"context\"--and specifies the direction it occurs (to the left or right of the word).\n","\n","def count_directional_context(data, word_representations):\n","    for i, word in enumerate(data):\n","        if word not in word_representations:\n","            continue\n","        start=i-window if i-window > 0 else 0\n","        end=i+window+1 if i+window+1 < len(data) else len(data)\n","        left=\"L: %s\" % ' '.join(data[start:i])\n","        right=\"R: %s\" % ' '.join(data[i+1:end])\n","        \n","        word_representations[word][left]+=1\n","        word_representations[word][right]+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ZYqIrqgJuLD"},"source":["# normalize a word represenatation vector that its L2 norm is 1.\n","# we do this so that the cosine similarity reduces to a simple dot product\n","\n","def normalize(word_representations):\n","    for word in word_representations:\n","        total=0\n","        for key in word_representations[word]:\n","            total+=word_representations[word][key]*word_representations[word][key]\n","            \n","        total=math.sqrt(total)\n","        for key in word_representations[word]:\n","            word_representations[word][key]/=total\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M8WrucERJuLD"},"source":["def dictionary_dot_product(dict1, dict2):\n","    dot=0\n","    for key in dict1:\n","        if key in dict2:\n","            dot+=dict1[key]*dict2[key]\n","    return dot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaMJS4hFJuLE"},"source":["def find_sim(word_representations, query):\n","    if query not in word_representations:\n","        print(\"'%s' is not in vocabulary\" % query)\n","        return None\n","    \n","    scores={}\n","    for word in word_representations:\n","        cosine=dictionary_dot_product(word_representations[query], word_representations[word])\n","        scores[word]=cosine\n","    return scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lis-QSRzJuLF"},"source":["# Find the K words with highest cosine similarity to a query in a set of word_representations\n","def find_nearest_neighbors(word_representations, query, K):\n","    scores=find_sim(word_representations, query)\n","    if scores != None:\n","        sorted_x = sorted(scores.items(), key=operator.itemgetter(1), reverse=True)\n","        for idx, (k, v) in enumerate(sorted_x[:K]):\n","            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-e0z3kGwJuLH"},"source":["# Let's find the contexts shared between two words that have the most contribution\n","# to the cosine similarity\n","\n","def find_shared_contexts(word_representations, query1, query2, K):\n","    if query1 not in word_representations:\n","        print(\"'%s' is not in vocabulary\" % query1)\n","        return None\n","    \n","    if query2 not in word_representations:\n","        print(\"'%s' is not in vocabulary\" % query2)\n","        return None\n","    \n","    context_scores={}\n","    dict1=word_representations[query1]\n","    dict2=word_representations[query2]\n","    \n","    for key in dict1:\n","        if key in dict2:\n","            score=dict1[key]*dict2[key]\n","            context_scores[key]=score\n","\n","    sorted_x = sorted(context_scores.items(), key=operator.itemgetter(1), reverse=True)\n","    #https://docs.python.org/3.6/howto/sorting.html - for more info on sorted()\n","    \n","    for idx, (k, v) in enumerate(sorted_x[:K]):\n","        print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"URRLRsMIJuLI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QJCuFQ58JuLJ"},"source":["1. Fill out a function `scale_tfidf` below.  This function takes as input a dict of word_representations and scales the value for each context in `word_representations[word]` by its tf-idf score.  Use the term frequency for tf and ${N \\over |\\{d \\in D : t \\in d\\}|}$ for idf.  \n","\n","Here, tf measure the count of a *context* term for a particular *word*, and idf measures the number of distinct *words* a particular *context* is seen with.  This function should modify `word_representations` in place."]},{"cell_type":"code","metadata":{"id":"lG8HtANrJuLJ"},"source":["def scale_tfidf(word_representations):            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lccjiPfcJuLK"},"source":["tf_idf_word_representations=create_vocab(wiki_data)\n","#pipeline for corpus applying tf-idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"21bLYvYZJuLL"},"source":["word_representations=create_vocab(wiki_data)\n","#pipeline for corpus without tf-idf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQ8GB1AgJuLM"},"source":["1. Compare the results the results of tf-idf scaling with the non-scaled results above.  How does scaling change the quality of the nearest neighbors, or the sensibility of the significant contexts?  Provide examples to support your claims using `find_nearest_neighbors` and `find_shared_contexts`."]},{"cell_type":"code","metadata":{"id":"g_znYtWqJuLM"},"source":[""],"execution_count":null,"outputs":[]}]}