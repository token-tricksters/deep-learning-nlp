{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment 07 - LSTM and biLSTM.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"h7DhiBrjbDoP","colab_type":"text"},"source":["# Assignment 07 - LSTM and biLSTM\n","This laboratory explores the concepts in Long Short-Term Memory (LSTM) and bidirectional LSTM (biLSTM)"]},{"cell_type":"code","metadata":{"id":"6Oc3FsVqRo42","colab_type":"code","colab":{}},"source":["!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1k4IFWScWnD3senYOUeVQFRv-6bOsmwWY' -O gdata.zip\n","!unzip -q ./gdata.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zf3oWL-_bDoR","colab_type":"text"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"_XDuafkjg2dy","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"NEXPPpnAbDoS","colab_type":"text"},"source":["## Part A"]},{"cell_type":"markdown","metadata":{"id":"zTs4E4l1bDoU","colab_type":"text"},"source":["1. Using the same `wonderland.txt` create a two models using a different LSTM configuration to compare their results when we tweak the parameters just a little.\n","    \n","    * Add two more layers, one with 256 and another with 512 units each\n","    * Use drop out of 0.25 and 0.30 for each layers\n","    * Consider a batch training of 64 instead of 128\n","    * Increase the number of epochs to 50\n","    * Choose a different optimzer than `adam`\n","    \n","Save the model with the lowest loss and use to generate your text for the next 1000-2000 chars.\n","\n","2. Using the training corpus from *Laboratory 06* (`./gdata/2016-us-presidential-debates.zip`) let us verify how our language models are affected depending on the chosen architecture. Using a LSTM architecture, create a language model considering the best hyperameters you obtained in question 01. Compare the generated text of the LSTM architecture with the one from *Laboratory 06*. \n","    * Does the generated text is closer to a human?\n","    * The approximate candidate from the previous model (Lab06) is the same one for the model generated in this question?\n","    * What factors contributed for the differences from one model to another?\n","    \n","Save the model with the lowest loss and use to generate your text for the next 2000 chars. Make sure to include the text from the model generated in Lab 06 (this is why we asked you to save the model in the previous assignment) so we can compare both models.   \n","\n","Specify the architecture and the configuration used for each model so we can compare how the parameters affected your results. Remember anyone should be able to reproduce your results!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fKQk9ltgbDoV","colab_type":"text"},"source":["## Part B"]},{"cell_type":"markdown","metadata":{"id":"phdMOz8FbDoW","colab_type":"text"},"source":[" 1. Download the dataset [Reuter_50_50 Dataset](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50) or open the zip in `./gdata/C50.zip`. This dataset is well divided with 50 authors with 50 documents each (train and test).\n","     * Make sure to convert your data into a tensor. this will make possible to re-use almost all code from our class.\n","     * Select two authors and train a binary classifier using either a LSTM or biLSTM\n","     * The preprocessing will change from our lab, but the core Sequential layers should be really similar.\n","     * Feel free to adjust your hyperparameters.\n","     \n","Specify the architecture and the configuration used for each model so we can compare how the parameters affected your results. Remember anyone should be able to reproduce your results!\n","     "]},{"cell_type":"markdown","metadata":{"id":"fWmGPgEtbDoX","colab_type":"text"},"source":["## Part C (Optional)"]},{"cell_type":"markdown","metadata":{"id":"tWrZxtUBbDoX","colab_type":"text"},"source":["1. Using the same dataset from the previous exercise (`./gdata/C50.zip`) use all 50 classes (all authors) and develop a multi-class classification using any desired architecture.\n","\n","Specify the architecture and the configuration used for each model so we can compare how the parameters affected your results. Remember anyone should be able to reproduce your results!"]},{"cell_type":"markdown","metadata":{"id":"-gEFsmFAbDoY","colab_type":"text"},"source":["."]},{"cell_type":"code","metadata":{"id":"Eo4QkjrPbDoZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}